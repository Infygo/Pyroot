{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard is a vis tool to work with tensorflow to visualise various aspects of the model \n",
    "> https://www.tensorflow.org/tensorboard/get_started  - official tutorial \n",
    "> https://www.tensorflow.org/tensorboard/\n",
    "> used the colab file and successfully created the logdir files - then downloaded that folder and uploaded to this notebook - make sure to change the virtual environment to tfgpu -  conda command prompt and triggered tensorboard --logdir logs\\fit \n",
    "> http://localhost:6006/ - localhost for tensorboard \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancer data \n",
    "df = pd.read_csv('../DATA/cancer_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('benign_0__mal_1',axis=1).values\n",
    "y = df['benign_0__mal_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and transform the data \n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the training data \n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the train data and test data \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation \n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the earlystop \n",
    "early_stop  = EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\vravindran\\\\Desktop\\\\Pyroot\\\\Machine learning_bootcamp\\\\Refactored_Py_DS_ML_Bootcamp-master\\\\22-Deep Learning\\\\TensorFlow_FILES\\\\ANNs'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Tensorboard callback "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Tensorboard Callback\n",
    "TensorBoard is a visualization tool provided with TensorFlow.\n",
    "\n",
    "This callback logs events for TensorBoard, including:\n",
    "\n",
    "Metrics summary plots\n",
    "Training graph visualization\n",
    "Activation histograms\n",
    "Sampled profiling\n",
    "If you have installed TensorFlow with pip, you should be able to launch TensorBoard from the command line:\n",
    "\n",
    "tensorboard --logdir=path_to_your_logs\n",
    "You can find more information about TensorBoard here.\n",
    "\n",
    "Arguments:\n",
    "    log_dir: the path of the directory where to save the log files to be\n",
    "      parsed by TensorBoard.\n",
    "    histogram_freq: frequency (in epochs) at which to compute activation and\n",
    "      weight histograms for the layers of the model. If set to 0, histograms\n",
    "      won't be computed. Validation data (or split) must be specified for\n",
    "      histogram visualizations.\n",
    "    write_graph: whether to visualize the graph in TensorBoard. The log file\n",
    "      can become quite large when write_graph is set to True.\n",
    "    write_images: whether to write model weights to visualize as image in\n",
    "      TensorBoard.\n",
    "    update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`,\n",
    "      writes the losses and metrics to TensorBoard after each batch. The same\n",
    "      applies for `'epoch'`. If using an integer, let's say `1000`, the\n",
    "      callback will write the metrics and losses to TensorBoard every 1000\n",
    "      samples. Note that writing too frequently to TensorBoard can slow down\n",
    "      your training.\n",
    "    profile_batch: Profile the batch to sample compute characteristics. By\n",
    "      default, it will profile the second batch. Set profile_batch=0 to\n",
    "      disable profiling. Must run in TensorFlow eager mode.\n",
    "    embeddings_freq: frequency (in epochs) at which embedding layers will\n",
    "      be visualized. If set to 0, embeddings won't be visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-03-20--2125'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datetime to create folder for different runs \n",
    "import datetime\n",
    "datetime.datetime.now().strftime(\"%Y-%m-%d--%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs\\fit\\20200320-212502\n",
      "<module 'ntpath' from 'C:\\\\Users\\\\vravindran\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\ntpath.py'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.join('logs','fit',datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),))\n",
    "print(os.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#log_directory = \"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d--%H%M\")\n",
    "log_dir1=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir4 = \"logs\\\\fit\\\\\"\n",
    "\n",
    "tensorboard_callback= tf.keras.callbacks.TensorBoard(log_dir=log_dir4, \n",
    "                                                     histogram_freq=1,\n",
    "                                                     write_graph=True,\n",
    "                                                     write_images=True,\n",
    "                                                     update_freq='epoch',\n",
    "                                                     profile_batch=2,\n",
    "                                                     embeddings_freq=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#BASE_DATA_PATH = 'C:/Grewe/Classes/CS663/Mat/LSTM/data'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and layers \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=30,activation='relu')) # Input layer \n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=15, activation='relu')) # Hidden layer \n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=1,activation='sigmoid')) # output layer \n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=600,\n",
    "          validation_data=(X_test, y_test), verbose=1,\n",
    "          callbacks=[early_stop, tensorboard_callback]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
