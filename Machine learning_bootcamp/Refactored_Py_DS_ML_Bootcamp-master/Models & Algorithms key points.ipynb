{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dataset - USA Housing # predicts - Housing price \n",
    "> Goal is to minimise the vertical distance between all data points and the line that pass through least square method \n",
    "> House price prediction - USA housing dataset \n",
    "> y label = ['Price'] => to be predicted \n",
    "> test train split \n",
    "> from sklearn.linear_model import Linear regression \n",
    "> instantiate the model \n",
    "> fit the model with X_train , y_train \n",
    "> intercept the model => lm.intercept_\n",
    "> coeff data - lm.coeff_\n",
    "> put the coeff data in a dataframe \n",
    "> predict the data => lm.predict(X_test)\n",
    "> Evaluate - y_test-predict - normal distribution of this plot represents a good prediction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dataset - Titanic # predicts - survived or died\n",
    "> model used for classification problems \n",
    "> Binary 0 or 1 \n",
    "> uses a sigmoid function resulting in the prediction to be either 0 or 1 #\n",
    "> Implement using the solution of linear regression model and fit it into logistic regression model \n",
    "> cut off point for the assigning of data to 0 or 1 class \n",
    "> Data cleaning - replacing NaN values of Age with avg age based on the passenger class \n",
    "> converting categorical columns into dummy variables => ex: Sex,embark column , drop the first column     in dummy conversion \n",
    "> concat the converted dummy columns into the dataframe \n",
    "> drop the useless columns like name, ticket and drop the original converted dummy column\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K nearest neighbours "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dataset - Not a clearly defined dataset - interview samples mostly \n",
    "> Consider a horse & dog sample plot \n",
    "> KNN - training algorithm , prediction algorithm \n",
    "> Training algorithm - stores all the data points \n",
    "> Prediction algorithm - calculates the distance of the to be predicted data to all other data points \n",
    "                       - sorts the points based on the distance \n",
    "                       - predicts based on the majority label of K closest points(as in sees whats the                            major class of data points are around the test data and puts in that class                              accordingly) \n",
    "> K value will impact the decision of which class the data point belongs to \n",
    "> from sklearn.preprocessing import StandardScaler \n",
    "> Instantiate the scaler \n",
    "> fit with the scaler - except the column that needs to be predicted \n",
    "> scaled feature => using transform feature transform the data frame into a scaled feature with columns                     except the column to be predicted \n",
    "> convert the scaled feature into a dataframe \n",
    "> use this dataframe to train test split the data into X & y labels \n",
    "> from sklearn.neighbours import KneighborsClassifier \n",
    "> instantiate the model with the neigbours (the types of classification)\n",
    "> fit the data \n",
    "> predict the data \n",
    "> Evaluate \n",
    "> Choosing the right K - Elbow method \n",
    "  -use a for loop with different K values and run the model \n",
    "  -in the loop determine the mean error np.mean(pred_i!=y_test)\n",
    "  -plot the Error rate against different K values \n",
    "> choose the K value with minimum error rate and rerun the model \n",
    "> Evaluate the model using metrics \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees and Random forests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dataset - kyphosis # predict - kyphosis or not\n",
    "> Most powerful classifier - use this for all problem sets \n",
    "> Example whether my friends come to visit based on weather \n",
    "> Nodes - columns/ factors used - features that can be split \n",
    "> Edges - outcome of split \n",
    "> Root - node that performs the first split \n",
    "> leaves - terminal nodes that predict the outcome \n",
    "> from sklearn.tree import DecisionTreeClassifier \n",
    "> test train split the data \n",
    "> instantiate the model \n",
    "> fit the training data \n",
    "> predict the data \n",
    "> Evaluate \n",
    "\n",
    "> Random forests - Since Decision trees has high variance \n",
    "> A new random sample of features is chosen for every single tree at every split \n",
    "> from sklearn.ensemble import RandomForestClassifier \n",
    "> instantiate \n",
    "> fit the data \n",
    "> predict the data \n",
    "> Evaluate \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dataset - cancer # predict - tumor is benign or malign\n",
    "> SVM - Non probablistic binary linear classifier \n",
    "> used for classification and regression \n",
    "> Data points are mapped in space making examples of separate categories divided by clear gap as wide as   possible \n",
    "> data points to be predicted are then mapped into this space and see which side of the gap they fall \n",
    "> Example dataset - Blue and pink class- a line that provides a separation \n",
    "                    Lots of options of hyperplanes that can separate blue and pink training points \n",
    "                    choose the hyper plane based on the maximising the margin between two classes \n",
    "> vector points that touches the margin lines are support vectors \n",
    "\n",
    "> from sklearn.svm import SVC\n",
    "> instantiate the model \n",
    "> train test split \n",
    "> predict \n",
    "> use the grid search to optimise the SVM \n",
    "> find the best C and gamma value \n",
    "> from sklearn.model_selection import GridSearchCV\n",
    "> define params with C and gamma list \n",
    "> instantiate the model\n",
    "> fit the model \n",
    "> best_params_\n",
    "> best_estimator_\n",
    "> rerun the model with Best C and gamma \n",
    "> predict \n",
    "> Evaluate \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
